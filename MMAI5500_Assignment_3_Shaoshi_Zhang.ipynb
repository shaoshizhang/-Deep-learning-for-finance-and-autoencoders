{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MMAI5500 Assignment #3",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqjwuG1sdldU"
      },
      "source": [
        "import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAVHKVglZ3Q5"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC-VpBmsZ5lI"
      },
      "source": [
        "# Size of our encoded representations\n",
        "encoding_sz = 5\n",
        "epochs = 150\n",
        "batch_sz = 32\n",
        "n_most_communal = 5\n",
        "n_least_communal = 20\n",
        "lambdas = np.arange(0.05, 0.21, 0.01)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-WqdF6xdoWR"
      },
      "source": [
        "define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_slSp-nZ7xQ"
      },
      "source": [
        "def modify_target(target, threshold=-0.05):\n",
        "    \"\"\"\n",
        "    threshold has to be negative\n",
        "    \"\"\"\n",
        "    cols = target.columns[1:]\n",
        "    # Get the returns\n",
        "    returns = target[cols].pct_change(axis=1)\n",
        "    # Remove drawdowns less than threshold\n",
        "    returns[returns < threshold] = np.abs(threshold)\n",
        "    # Reconstruct\n",
        "    target_mod = target.copy()\n",
        "    target_mod[cols] = returns\n",
        "    target_mod[cols[0]] = target[cols[0]]\n",
        "\n",
        "    for i, col in enumerate(cols[1:]):\n",
        "        target_mod[col] = target_mod[cols[i]] * (target_mod[col] + 1)\n",
        "\n",
        "    return target_mod"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwWYudr_Z9od"
      },
      "source": [
        "def load_modify_normalize(data_fname, target_fname):\n",
        "    \"\"\"\n",
        "    Loads, modifies (the IBB index only) and normalizes the data.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "        data_fname      - file name (including path) for the data file\n",
        "                          (e.g. assign3_data.csv)\n",
        "        target_fname    - file name (including path) for the file containing\n",
        "                          the IBB index (e.g. assign3_benchmark.csv)\n",
        "    Returns\n",
        "    -------\n",
        "        X_train         - shape: (n_stocks * 4, n_times), training data,\n",
        "                          normalized stock prices.\n",
        "        X_valid         - shape: (n_stocks, n_times), validation data,\n",
        "                          normalized stock prices.\n",
        "        Y_train         - shape: (n_times * 4, 1), training data,\n",
        "                          normalized IBB index.\n",
        "        Y_valid         - shape: (n_times, 1), validation data,\n",
        "                          normalized IBB index.\n",
        "        Y_train_mod     - shape: (n_times * 4, 1), training data,\n",
        "                          modified and normalized IBB index.\n",
        "        Y_valid_mod     - shape: (n_times * 4, 1), validation data,\n",
        "                          modified and normalized IBB index.\n",
        "        tickers         - List of the ticker symbols\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(data_fname, index_col=0)\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    min_max_scaler.fit(data.values[:, 1:].T)\n",
        "    data.values[:, 1:] = min_max_scaler.transform(data.values[:, 1:].T).T\n",
        "    X_train = data[data['year'] < 2020].values[:, 1:]\n",
        "    X_valid = data[data['year'] >= 2020].values[:, 1:]\n",
        "    tmp = data.index[data['year'] == 2020.]\n",
        "    # List of the ticker symbols\n",
        "    tickers = np.array([ticker.rstrip('_2020') for ticker in tmp])\n",
        "\n",
        "    # Benchmark, i.e. target, IBB\n",
        "    target = pd.read_csv(target_fname, index_col=0)\n",
        "    # Modify the target to remove drawdowns\n",
        "    target_mod = modify_target(target, threshold=-0.05)\n",
        "    # Rescale the traget\n",
        "    min_max_scaler.fit(target.values[:, 1:].T)\n",
        "    target.values[:, 1:] = min_max_scaler.transform(target.values[:, 1:].T).T\n",
        "    target_mod.values[:, 1:] = \\\n",
        "        min_max_scaler.transform(target_mod.values[:, 1:].T).T\n",
        "    # Split into train and valid\n",
        "    Y_train = target[target['year'] < 2020].values[:, 1:]\n",
        "    Y_valid = target[target['year'] >= 2020].values[:, 1:]\n",
        "    Y_train_mod = target_mod[target_mod['year'] < 2020].values[:, 1:]\n",
        "    Y_valid_mod = target_mod[target_mod['year'] >= 2020].values[:, 1:]\n",
        "    # Reshape the Y_train_mod_n to (n_times*4, 1)\n",
        "    # & Y_valid_mod_n to (n_times, 1)\n",
        "    Y_train_mod = Y_train_mod.reshape(-1, 1)\n",
        "    Y_valid_mod = Y_valid_mod.reshape(-1, 1)\n",
        "\n",
        "    return X_train, X_valid, Y_train, Y_valid, Y_train_mod, Y_valid_mod, tickers"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8Qr-LNWaBY6"
      },
      "source": [
        "def build_autoencoder(lmbd, n_times):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(n_times,))\n",
        "    # \"encoded\" is the encoded representation of the input\n",
        "    encoded = layers.Dense(encoding_sz, activation='relu',\n",
        "                           kernel_regularizer=regularizers.l2(lmbd))(inputs)\n",
        "    # \"decoded\" is the lossy reconstruction of the input\n",
        "    decoded = layers.Dense(n_times, activation='sigmoid',\n",
        "                           kernel_regularizer=regularizers.l2(lmbd))(encoded)\n",
        "\n",
        "    # This model maps an input to its reconstruction\n",
        "    autoencoder = keras.Model(inputs, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return autoencoder"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeLC7vYPaDcy"
      },
      "source": [
        "def train_autoencoder(lambdas, X_train, X_valid):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    n_times = X_valid.shape[1]\n",
        "    val_losses = []\n",
        "    best_epochs = []\n",
        "    for lmbd in lambdas:\n",
        "\n",
        "        autoencoder = build_autoencoder(lmbd, n_times)\n",
        "\n",
        "        history = autoencoder.fit(X_train, X_train, epochs=epochs,\n",
        "                                  batch_size=batch_sz, shuffle=True,\n",
        "                                  validation_data=(X_valid, X_valid),\n",
        "                                  verbose=0)\n",
        "\n",
        "        min_val_loss = np.min(history.history['val_loss'])\n",
        "        best_epochs.append(np.argmin(history.history['val_loss'])+1)\n",
        "\n",
        "        if (min_val_loss < val_losses).all():\n",
        "            best_history = history.history\n",
        "            best_lambda = lmbd\n",
        "            best_epoch = best_epochs[-1]\n",
        "\n",
        "        val_losses.append(min_val_loss)\n",
        "\n",
        "        return val_losses, best_history, best_lambda, best_epoch"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XefkdmXWaF7Y"
      },
      "source": [
        "def select_portfolio(X_train, X_valid, best_lambda, best_epoch,\n",
        "                     tickers, n_most_communal=5, n_least_communal=20):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    n_times = X_valid.shape[1]\n",
        "    autoencoder = build_autoencoder(best_lambda, n_times)\n",
        "    autoencoder.fit(X_train, X_train, epochs=best_epoch,\n",
        "                    batch_size=batch_sz, shuffle=True, verbose=0)\n",
        "\n",
        "    losses = np.zeros(X_valid.shape[0])\n",
        "    for i, x in enumerate(X_valid):\n",
        "        x = x.reshape((1, -1))\n",
        "        losses[i] = autoencoder.evaluate(x, x, verbose=0)\n",
        "\n",
        "    ids = np.argsort(losses)\n",
        "    most_communal_ids = ids[:n_most_communal]\n",
        "    least_communal_ids = ids[-n_least_communal:]\n",
        "    communal_tickers = {'most': tickers[most_communal_ids],\n",
        "                        'least': tickers[least_communal_ids]}\n",
        "    portfolio_ids = np.r_[most_communal_ids, least_communal_ids]\n",
        "\n",
        "    # Transpose the data since now a sample will be the\n",
        "    # portfolio at a single time point.\n",
        "    X_valid_port = X_valid[portfolio_ids].T\n",
        "    # In X_train the stocks are repreated 4 times\n",
        "    X_train_port = []\n",
        "    for i in range(1, 5):\n",
        "        X_train_port.append(X_train[portfolio_ids * i].T)\n",
        "    X_train_port = np.concatenate(X_train_port)\n",
        "\n",
        "    return X_train_port, X_valid_port, portfolio_ids, communal_tickers"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UUWri9Odq96"
      },
      "source": [
        "load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMpjhk_8aIkm"
      },
      "source": [
        "X_train, X_valid, Y_train, Y_valid, Y_train_mod, Y_valid_mod, tickers = load_modify_normalize('assign3_data.csv', 'assign3_benchmark.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbsqNjtsdwAW"
      },
      "source": [
        " select stocks for the portfolio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06SWTyA6aVDM"
      },
      "source": [
        "val_losses, best_history, best_lambda, best_epoch = train_autoencoder(lambdas, X_train, X_valid)\n",
        "print('val_losses: \\n', val_losses)\n",
        "print('best_history: \\n', best_history)\n",
        "print('best_lambda: \\n', best_lambda)\n",
        "print('best_epoch: \\n', best_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4crJqcjfbYrU"
      },
      "source": [
        "X_train_port, X_valid_port, portfolio_ids, communal_tickers = select_portfolio(X_train, X_valid, best_lambda, best_epoch, tickers)\n",
        "print('X_train_port: \\n', X_train_port)\n",
        "print('X_valid_port: \\n', X_valid_port)\n",
        "print('portfolio_ids: \\n', portfolio_ids)\n",
        "print('communal_tickers_most: \\n', communal_tickers['most'])\n",
        "print('communal_tickers_least: \\n', communal_tickers['least'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piv9jHbMdxtb"
      },
      "source": [
        "train a linear regression model using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaRsqCbSgsNn"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "model = Sequential()\n",
        "model.add(Dense(encoding_sz, input_dim=X_train_port.shape[1], activation='relu'))\n",
        "model.add(Dense(Y_train_mod.shape[1], activation='linear'))\n",
        "model.compile(loss='mse', optimizer='sgd', metrics=['mse'])\n",
        "history = model.fit(X_train_port, Y_train_mod, batch_size=batch_sz, epochs=epochs, shuffle=False,\n",
        "                    validation_data=(X_valid_port, Y_valid_mod))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRxO6BliQGt"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "plt.plot(history_df['loss'], label='loss')\n",
        "plt.plot(history_df['val_loss'], label='val_loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUiBY63dk0Fn"
      },
      "source": [
        "predict the portfolio performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q9LjB8biv5E"
      },
      "source": [
        "Y_valid_predict = model.predict(X_valid_port)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POp97Ddvqkz2"
      },
      "source": [
        "Plot the predicted performance and compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MsnL792oDWn"
      },
      "source": [
        "plt.plot(Y_valid.reshape(-1, 1), label='unmodified')\n",
        "plt.plot(Y_valid_mod, label='modified')\n",
        "plt.plot(Y_valid_predict, label='predict')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}